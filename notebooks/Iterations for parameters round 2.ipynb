{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.knn import KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14,8)\n",
    "#plt.rcParams['figure.dpi'] = 150\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = 'https://raw.githubusercontent.com/Xiaoqi-Sun/aptamer_scoring/main/'\n",
    "\n",
    "# raw data\n",
    "for i in np.arange(2,7):\n",
    "    exec(\"R{}E = pd.read_csv(repo_url+'serotonin%20raw%20data/{}RE.csv')\".format(i, i))\n",
    "    exec(\"R{}C = pd.read_csv(repo_url+'serotonin%20raw%20data/{}RC.csv')\".format(i, i))\n",
    "    \n",
    "# processed data\n",
    "for i in np.arange(2,7):\n",
    "    exec(\"R{}E_frequency = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}E_frequency.csv',index_col='Quadrumer')\".format(i, i))\n",
    "    exec(\"R{}C_frequency = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}C_frequency.csv',index_col='Quadrumer')\".format(i, i))\n",
    "    exec(\"R{}E_full_table_weighted = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n",
    "    exec(\"R{}C_full_table_weighted = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental dF/F values\n",
    "\n",
    "# round 1: Sanghwa's 80 sequences\n",
    "dFF = pd.read_csv(repo_url+'dFF%20data/dFF_r1.csv',usecols=[0,1,2])\n",
    "\n",
    "# round 2: Xiaoqi's Prediction\n",
    "dFF2_new = pd.read_csv(repo_url+'dFF%20data/dFF_r2_new.csv')\n",
    "#dFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\n",
    "#dFF2_old = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\n",
    "\n",
    "# round 3: Payam's prediction\n",
    "#dFF3_old = pd.read_csv(repo_url+'dFF%20data/dFF_r3_old.csv',usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "dFF3_new = pd.read_csv(repo_url+'dFF%20data/dFF_r3_new.csv',usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function for calculating quad score\n",
    "\n",
    "def max_freq_ratio(quad_seq):\n",
    "    #find the last term of the score definition\n",
    "    r2=R2E_frequency[R2E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2E_frequency.index else 0\n",
    "    r3=R3E_frequency[R3E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3E_frequency.index else 0\n",
    "    r4=R4E_frequency[R4E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4E_frequency.index else 0\n",
    "    r5=R5E_frequency[R5E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5E_frequency.index else 0      \n",
    "    r6=R6E_frequency[R6E_frequency.index==quad_seq]['Weighted frequency'][0]\n",
    "    \n",
    "    # handle the inf case\n",
    "    r6r5 = 0 if r5==0 else r6/r5\n",
    "    r5r4 = 0 if r4==0 else r5/r4\n",
    "    r4r3 = 0 if r3==0 else r4/r3\n",
    "    r3r2 = 0 if r3==0 else r3/r2\n",
    "    return max(r3r2,r4r3,r5r4,r6r5)\n",
    "\n",
    "def max_freq_ratio_ctrl(quad_seq):\n",
    "    #find the last term of the score definition\n",
    "    r2=R2C_frequency[R2C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2C_frequency.index else 0\n",
    "    r3=R3C_frequency[R3C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3C_frequency.index else 0\n",
    "    r4=R4C_frequency[R4C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4C_frequency.index else 0\n",
    "    r5=R5C_frequency[R5C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5C_frequency.index else 0      \n",
    "    r6=R6C_frequency[R6C_frequency.index==quad_seq]['Weighted frequency'][0]\n",
    "    \n",
    "    # handle the inf case\n",
    "    r6r5 = 0 if r5==0 else r6/r5\n",
    "    r5r4 = 0 if r4==0 else r5/r4\n",
    "    r4r3 = 0 if r3==0 else r4/r3\n",
    "    r3r2 = 0 if r3==0 else r3/r2\n",
    "    \n",
    "    return max(r3r2,r4r3,r5r4,r6r5)\n",
    "\n",
    "def extract_quadrumers(aptamer_sequence):\n",
    "    #takes in one 18-mer and return a table of quadrumers, with a position column and a quadrumer column\n",
    "    quadrumers = []\n",
    "    for i in np.arange(15):\n",
    "        quad = aptamer_sequence[i:i+4]\n",
    "        quadrumers = np.append(quadrumers,quad)\n",
    "    return quadrumers\n",
    "\n",
    "\n",
    "def quad_score_exp(set_1_percentile, exp_ind_weight, name):\n",
    "    #inputs: set_1_percentile -> see definition of set 1; {exp,ctrl}weight: weight for two indicator functions\n",
    "    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n",
    "    #using quadrumers in R6E for calculation \n",
    "    \n",
    "    #set 1: Kmers with frequencies once greater than 99.5th percentile of the kmers in the control round\n",
    "    control_percentile = np.percentile(R2E_frequency['Weighted frequency'], set_1_percentile)\n",
    "    set1 = R6E_frequency[R6E_frequency['Weighted frequency']>control_percentile]\n",
    "    set1_list = set1.index\n",
    "\n",
    "    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n",
    "    set2 = R2E_frequency.merge(R6E_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2E freq', 'Weighted frequency_y':'R6E freq'})\n",
    "    set2['amp-fold value'] = set2['R6E freq']/set2['R2E freq']\n",
    "    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n",
    "    set2_list = set2.index\n",
    "    \n",
    "    score_r6 = []\n",
    "    for i in R6E_frequency.index:\n",
    "        term1 = (i in set1_list) or (i in set2_list)\n",
    "        term2 = (i in set1_list)\n",
    "        term3 = max_freq_ratio(i)\n",
    "        score_r6 = np.append(score_r6, term1*exp_ind_weight + term2*exp_ind_weight + term3)\n",
    "        \n",
    "    R6E_with_score = R6E_frequency.copy()\n",
    "    R6E_with_score[name+'_exp'] = score_r6\n",
    "    return R6E_with_score\n",
    "\n",
    "def quad_score_ctrl(set_1_percentile, ctrl_ind_weight, name):\n",
    "    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n",
    "    #using quadrumers in R6E for calculation \n",
    "    \n",
    "    #set 1 NOTE: using 99.5 percentile only gives 3 quadrumers\n",
    "    control_percentile = np.percentile(R2C_frequency['Weighted frequency'],set_1_percentile) \n",
    "    set1 = R6C_frequency[R6C_frequency['Weighted frequency'] > control_percentile]\n",
    "    set1_list = set1.index\n",
    "    \n",
    "    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n",
    "    set2 = R2C_frequency.merge(R6C_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2C freq', 'Weighted frequency_y':'R6C freq'})\n",
    "    set2['amp-fold value'] = set2['R6C freq']/set2['R2C freq']\n",
    "    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n",
    "    set2_list = set2.index\n",
    "    \n",
    "    score_r6 = []\n",
    "    for i in R6C_frequency.index:\n",
    "        term1 = (i in set1_list) or (i in set2_list)\n",
    "        term2 = (i in set1_list)\n",
    "        term3 = max_freq_ratio_ctrl(i)\n",
    "        score_r6 = np.append(score_r6, term1*ctrl_ind_weight + term2*ctrl_ind_weight + term3)\n",
    "        \n",
    "    R6C_with_score = R6C_frequency.copy()\n",
    "    R6C_with_score[name+'_ctrl'] = score_r6\n",
    "    return R6C_with_score\n",
    "\n",
    "def quad_score_full(set_1_percentile_exp, set_1_percentile_ctrl, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n",
    "    quad_exp = quad_score_exp(set_1_percentile_exp, exp_ind_weight, name)\n",
    "    quad_ctrl = quad_score_ctrl(set_1_percentile_ctrl, ctrl_ind_weight, name)\n",
    "    \n",
    "    merged = quad_exp.merge(quad_ctrl, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    merged[name] = exp_weight*merged[name+ '_exp'] - ctrl_weight*merged[name+'_ctrl']\n",
    "    return pd.DataFrame({'Weighted frequency': merged['Weighted frequency_x'], #weighted frequence is from R6E\n",
    "                        name : merged[name] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions for calculating aptamer score \n",
    "\n",
    "def aptamer_score(RnE, quad_score, name):\n",
    "    # Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n",
    "    # Inputs: RnE: a dataframe with 18-mer sequences;\n",
    "    #        quad_score: a df , out put of quad_score_full function,\n",
    "    \n",
    "    quadrumer_score = quad_score\n",
    "    score_name = quadrumer_score.columns[1]\n",
    "    \n",
    "    aptamer_score = []\n",
    "    aptamer_freqsum = []\n",
    "    for apt_seq in RnE['Trimed']:\n",
    "        all_quads = extract_quadrumers(apt_seq)\n",
    "        one_score = 0\n",
    "        one_freqsum = 0\n",
    "        for quad in all_quads:\n",
    "            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n",
    "                one_score += quadrumer_score.loc[quad][1]\n",
    "                one_freqsum += quadrumer_score.loc[quad][0]\n",
    "        aptamer_score = np.append(aptamer_score, one_score)\n",
    "        aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n",
    "        \n",
    "    tbl_with_score = RnE.copy().loc[:,['Trimed']]\n",
    "    tbl_with_score[name]=aptamer_score\n",
    "    tbl_with_score['Weighted frequency'] = aptamer_freqsum\n",
    "    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n",
    "    \n",
    "    \n",
    "    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n",
    "    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n",
    "\n",
    "\n",
    "    return tbl_with_score\n",
    "\n",
    "\n",
    "def aptamer_score_dFF(dFF_tbl, quad_score, name):\n",
    "    # for incorporating all 80 sequences of dFF table\n",
    "    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n",
    "    #Inputs: RnE: a dataframe with 18-mer sequences;\n",
    "    #        quad_score: a df , out put of quad_score_full function,\n",
    "    quadrumer_score = quad_score\n",
    "    score_name = quadrumer_score.columns[1]\n",
    "    \n",
    "    aptamer_score = []\n",
    "    aptamer_freqsum = []\n",
    "    for apt_seq in dFF_tbl['Sequence']:\n",
    "        all_quads = extract_quadrumers(apt_seq)\n",
    "        one_score = 0\n",
    "        #one_freqsum = 0\n",
    "        for quad in all_quads:\n",
    "            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n",
    "                one_score += quadrumer_score.loc[quad][1]\n",
    "                #one_freqsum += quadrumer_score.loc[quad][0]\n",
    "        aptamer_score = np.append(aptamer_score, one_score)\n",
    "        \n",
    "    tbl_with_score = dFF_tbl.copy().loc[:,['Name','Sequence','dFF']]\n",
    "    tbl_with_score[name]=aptamer_score\n",
    "    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n",
    "    \n",
    "    \n",
    "    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n",
    "    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n",
    "\n",
    "\n",
    "    return tbl_with_score\n",
    "\n",
    "def dFF_with_score(threshold, dFF_tbl, quad_score, score_name):\n",
    "    dFF_with_score = aptamer_score_dFF(dFF_tbl, quad_score, name=score_name)\n",
    "    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n",
    "    return dFF_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations\n",
    "### Parameters in integer\n",
    "- exp_ind_weight: 1 - 10\n",
    "- ctrl_ind_weight: 1 - 10\n",
    "- exp_weight: 1 - 10 \n",
    "- ctrl_weight: 1- 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_OD(dFF_with_score_tbl, X_train, clf):\n",
    "    outlier_label = clf.fit(X_train).predict(X_train)\n",
    "    dFF_with_outlier = dFF_with_score_tbl.copy()\n",
    "    dFF_with_outlier['outlier'] = outlier_label\n",
    "    outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n",
    "    r_OD = outliers_dropped.corr().iloc[0,1] ## raw score and dFF\n",
    "    return r_OD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1 \n",
    "    all_exp_ind_weights.append(exp_ind_weight)\n",
    "                all_ctrl_ind_weights.append(ctrl_ind_weight)\n",
    "                all_exp_weights.append(exp_weight)\n",
    "                all_ctrl_weights.append(ctrl_weight)\n",
    "                all_overall_r.append(overall_r)\n",
    "                all_ABOD_r.append(ABOD_r)\n",
    "                all_KNN_r.append(KNN_r)\n",
    "                all_OCSVM_r.append(OCSVM_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2\n",
    "\n",
    "%%time\n",
    "all_exp_ind_weights = []\n",
    "all_ctrl_ind_weights = []\n",
    "all_exp_weights = []\n",
    "all_ctrl_weights = []\n",
    "all_overall_r = []\n",
    "all_ABOD_r = []\n",
    "all_KNN_r = []\n",
    "all_OCSVM_r = []\n",
    "n = 3 # range 1-10\n",
    "\n",
    "for exp_ind_weight in np.arange(1,n):\n",
    "    for ctrl_ind_weight in np.arange(1,n):\n",
    "        for exp_weight in np.arange(1,n):\n",
    "            for ctrl_weight in np.arange(1,n):\n",
    "                \n",
    "                # Set up DataFrames\n",
    "                temp_quads_score = quad_score_full(99.5, 99.5, exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight, 'temp')\n",
    "                dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name='temp')\n",
    "                X_train = dFF_with_score_tbl.loc[:,['dFF','temp su']].values\n",
    "\n",
    "                # overall_r\n",
    "                overall_r = dFF_with_score_tbl.corr().iloc[0,3]\n",
    "                \n",
    "                # Outlier detecting r\n",
    "                ABOD_r = r_OD(dFF_with_score_tbl, X_train, ABOD())\n",
    "                KNN_r = r_OD(dFF_with_score_tbl, X_train, KNN())\n",
    "                OCSVM_r = r_OD(dFF_with_score_tbl, X_train, OCSVM())\n",
    "                \n",
    "                # append to list\n",
    "                all_exp_ind_weights = np.append(all_exp_ind_weights, exp_ind_weight)\n",
    "                all_ctrl_ind_weights = np.append(all_ctrl_ind_weights, ctrl_ind_weight)\n",
    "                all_exp_weights = np.append(all_exp_weights, exp_weight)\n",
    "                all_ctrl_weights = np.append(all_ctrl_weights,ctrl_weight)\n",
    "                all_overall_r = np.append(all_overall_r, overall_r)\n",
    "                all_ABOD_r = np.append(all_ABOD_r, ABOD_r)\n",
    "                all_KNN_r = np.append(all_KNN_r, KNN_r)\n",
    "                all_OCSVM_r = np.append(all_OCSVM_r,OCSVM_r)\n",
    "                \n",
    "                if len(all_exp_ind_weights) == 1000:\n",
    "                    print(1000)\n",
    "                if len(all_exp_ind_weights) == 5000:\n",
    "                    print(5000)\n",
    "                    \n",
    "                    \n",
    "full_table = pd.DataFrame({'exp_ind_weight': all_exp_ind_weights,\n",
    "                             'ctrl_ind_weight': all_ctrl_ind_weights,\n",
    "                             'exp_weight':all_exp_weights,\n",
    "                             'ctrl_weight':all_ctrl_weights,\n",
    "                             'overall_r':all_overall_r,\n",
    "                             'ABOD_r':all_ABOD_r,\n",
    "                             'KNN_r': all_KNN_r,\n",
    "                             'OCSVM_r':all_OCSVM_r})\n",
    "\n",
    "full_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3\n",
    "\n",
    "filename = \"iteration_round_2.csv\"\n",
    "f = open(filename, 'w')\n",
    "with f:\n",
    "    csv_writer = csv.writer(f) \n",
    "    csv_writer.writerow([\"exp_ind_weight\", \"ctrl_ind_weight\", \"exp_weight\",\"ctrl_weight\", \"overall_r\", \"ABOD_all_r\", \"KNN_all_r\",\"overall_r_r1_only\"])\n",
    "    \n",
    "    n = 3 \n",
    "    \n",
    "    for exp_ind_weight in np.arange(1,n):\n",
    "        for ctrl_ind_weight in np.arange(1,n):\n",
    "            for exp_weight in np.arange(1,n):\n",
    "                for ctrl_weight in np.arange(1,n):\n",
    "\n",
    "                    # set up quadrumer score\n",
    "                    temp_quads_score = quad_score_full(99.5, 99.5, exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight, 'temp')\n",
    "                    \n",
    "                    # compute dFF score\n",
    "                    dFF_1_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF, quad_score=temp_quads_score, score_name='temp')\n",
    "                    dFF_2_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name='temp')\n",
    "                    dFF_3_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF3_new, quad_score=temp_quads_score, score_name='temp')\n",
    "                    dFF_combined = dFF_1_with_score_tbl.append(dFF_2_with_score_tbl).append(dFF_3_with_score_tbl)\n",
    "                    \n",
    "                    \n",
    "                    X_train = dFF_combined.loc[:,['dFF','temp']].values\n",
    "\n",
    "                    # overall_r\n",
    "                    overall_r = dFF_combined.corr().iloc[0,1]\n",
    "                  \n",
    "                    # Outlier detecting r for all sequences\n",
    "                    ABOD_all_r = r_OD(dFF_combined, X_train, ABOD())\n",
    "                    KNN_all_r = r_OD(dFF_combined, X_train, KNN())\n",
    "                    \n",
    "                    # Original round 1 r\n",
    "                    overall_r_r1_only = dFF_1_with_score_tbl.corr().iloc[0,1]\n",
    "                    \n",
    "                    \n",
    "                    # write to file\n",
    "                    csv_writer.writerow([exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, overall_r, ABOD_all_r, KNN_all_r,overall_r_r1_only])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full = pd.read_csv('iteration_round_2.csv')#.drop(columns=['Unnamed: 0'])\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP 10 Predictions (iterated version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ABOD_top10_para = full.sort_values('ABOD_r',ascending=False).head(10).loc[:,'exp_ind_weight':'ctrl_weight']\n",
    "KNN_top10_para = full.sort_values('KNN_r',ascending=False).head(10).loc[:,'exp_ind_weight':'ctrl_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_top_10_ABOD = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    # parameters\n",
    "    exp_percentile, ctrl_percentile = [99.5, 99.5]\n",
    "    exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = ABOD_top10_para.iloc[i]\n",
    "    name = 'score name'\n",
    "    \n",
    "    # temp tables\n",
    "    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\n",
    "    temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n",
    "    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n",
    "\n",
    "    # get top 10\n",
    "    temp_top_10 = temp_aptamer_score.sort_values('score name su', ascending=False).head(10)['Trimed']\n",
    "    all_top_10_ABOD = np.append(all_top_10_ABOD, temp_top_10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_10_KNN = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    # parameters\n",
    "    exp_percentile, ctrl_percentile = [99.5, 99.5]\n",
    "    exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = KNN_top10_para.iloc[i]\n",
    "    name = 'score name'\n",
    "    \n",
    "    # temp tables\n",
    "    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\n",
    "    temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n",
    "    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n",
    "\n",
    "    # get top 10\n",
    "    temp_top_10 = temp_aptamer_score.sort_values('score name su', ascending=False).head(10)['Trimed']\n",
    "    all_top_10_KNN = np.append(all_top_10_KNN, temp_top_10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the patterns of the 100 sequences from each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_top_10_ABOD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c9666d123541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_top_10_ABOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_top_10_ABOD' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(all_top_10_ABOD).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACAACCGCTCACTCCGAT    10\n",
       "ACCGACCACAACTCCGCT    10\n",
       "ACCAAGCACTCCGATCCT    10\n",
       "ACAACCCAACTCCGCTCG    10\n",
       "AACCGATCCAACCACTCG    10\n",
       "ACCAACACTCCGCTCGAT    10\n",
       "TACCACTCCAACTCCGCT    10\n",
       "ACTCCGAACCACTCCGCT    10\n",
       "ACCGCACAATCCTCCGAT     8\n",
       "CAACCAGAGCACTCCGAT     6\n",
       "AGCACTCCGATCCTCACA     4\n",
       "GGCACGCACCGATCCGAT     1\n",
       "ATCCGCAACTCATCCGCT     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(all_top_10_KNN).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_10_KNN = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
