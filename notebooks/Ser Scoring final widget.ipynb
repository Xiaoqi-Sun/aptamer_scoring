{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.knn import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14,8)\n",
    "#plt.rcParams['figure.dpi'] = 150\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = 'https://raw.githubusercontent.com/Xiaoqi-Sun/aptamer_scoring/main/'\n",
    "\n",
    "# raw data\n",
    "for i in np.arange(2,7):\n",
    "    exec(\"R{}E = pd.read_csv(repo_url+'serotonin%20raw%20data/{}RE.csv')\".format(i, i))\n",
    "    exec(\"R{}C = pd.read_csv(repo_url+'serotonin%20raw%20data/{}RC.csv')\".format(i, i))\n",
    "    \n",
    "# processed data\n",
    "for i in np.arange(2,7):\n",
    "    exec(\"R{}E_frequency = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}E_frequency.csv',index_col='Quadrumer')\".format(i, i))\n",
    "    exec(\"R{}C_frequency = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}C_frequency.csv',index_col='Quadrumer')\".format(i, i))\n",
    "    exec(\"R{}E_full_table_weighted = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n",
    "    exec(\"R{}C_full_table_weighted = pd.read_csv(repo_url+'serotonin%20processed%20data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental dF/F values\n",
    "\n",
    "# round 1: Sanghwa's 80 sequences\n",
    "dFF = pd.read_csv(repo_url+'dFF%20data/dFF_r1.csv',usecols=[0,1,2])\n",
    "\n",
    "# round 2: Xiaoqi's Prediction\n",
    "dFF2_new = pd.read_csv(repo_url+'dFF%20data/dFF_r2_new.csv')\n",
    "#dFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\n",
    "#dFF2_old = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\n",
    "\n",
    "# round 3: Payam's prediction\n",
    "#dFF3_old = pd.read_csv(repo_url+'dFF%20data/dFF_r3_old.csv',usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "dFF3_new = pd.read_csv(repo_url+'dFF%20data/dFF_r3_new.csv',usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "\n",
    "# round 4: Payam's  prediction (Model 3)\n",
    "dFF4_pos = pd.read_csv(repo_url+'dFF%20data/dFF_r4_positive.csv')\n",
    "dFF4_neg = pd.read_csv(repo_url+'dFF%20data/dFF_r4_negative.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function for calculating quad score\n",
    "\n",
    "def max_freq_ratio(quad_seq):\n",
    "    #find the last term of the score definition\n",
    "    r2=R2E_frequency[R2E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2E_frequency.index else 0\n",
    "    r3=R3E_frequency[R3E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3E_frequency.index else 0\n",
    "    r4=R4E_frequency[R4E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4E_frequency.index else 0\n",
    "    r5=R5E_frequency[R5E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5E_frequency.index else 0      \n",
    "    r6=R6E_frequency[R6E_frequency.index==quad_seq]['Weighted frequency'][0]\n",
    "    \n",
    "    # handle the inf case\n",
    "    r6r5 = 0 if r5==0 else r6/r5\n",
    "    r5r4 = 0 if r4==0 else r5/r4\n",
    "    r4r3 = 0 if r3==0 else r4/r3\n",
    "    r3r2 = 0 if r3==0 else r3/r2\n",
    "    return max(r3r2,r4r3,r5r4,r6r5)\n",
    "\n",
    "def max_freq_ratio_ctrl(quad_seq):\n",
    "    #find the last term of the score definition\n",
    "    r2=R2C_frequency[R2C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2C_frequency.index else 0\n",
    "    r3=R3C_frequency[R3C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3C_frequency.index else 0\n",
    "    r4=R4C_frequency[R4C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4C_frequency.index else 0\n",
    "    r5=R5C_frequency[R5C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5C_frequency.index else 0      \n",
    "    r6=R6C_frequency[R6C_frequency.index==quad_seq]['Weighted frequency'][0]\n",
    "    \n",
    "    # handle the inf case\n",
    "    r6r5 = 0 if r5==0 else r6/r5\n",
    "    r5r4 = 0 if r4==0 else r5/r4\n",
    "    r4r3 = 0 if r3==0 else r4/r3\n",
    "    r3r2 = 0 if r3==0 else r3/r2\n",
    "    \n",
    "    return max(r3r2,r4r3,r5r4,r6r5)\n",
    "\n",
    "def extract_quadrumers(aptamer_sequence):\n",
    "    #takes in one 18-mer and return a table of quadrumers, with a position column and a quadrumer column\n",
    "    quadrumers = []\n",
    "    for i in np.arange(15):\n",
    "        quad = aptamer_sequence[i:i+4]\n",
    "        quadrumers = np.append(quadrumers,quad)\n",
    "    return quadrumers\n",
    "\n",
    "\n",
    "def quad_score_exp(set_1_percentile, exp_ind_weight, name):\n",
    "    #inputs: set_1_percentile -> see definition of set 1; {exp,ctrl}weight: weight for two indicator functions\n",
    "    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n",
    "    #using quadrumers in R6E for calculation \n",
    "    \n",
    "    #set 1: Kmers with frequencies once greater than 99.5th percentile of the kmers in the control round\n",
    "    control_percentile = np.percentile(R2E_frequency['Weighted frequency'], set_1_percentile)\n",
    "    set1 = R6E_frequency[R6E_frequency['Weighted frequency']>control_percentile]\n",
    "    set1_list = set1.index\n",
    "\n",
    "    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n",
    "    set2 = R2E_frequency.merge(R6E_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2E freq', 'Weighted frequency_y':'R6E freq'})\n",
    "    set2['amp-fold value'] = set2['R6E freq']/set2['R2E freq']\n",
    "    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n",
    "    set2_list = set2.index\n",
    "    \n",
    "    score_r6 = []\n",
    "    for i in R6E_frequency.index:\n",
    "        term1 = (i in set1_list) or (i in set2_list)\n",
    "        term2 = (i in set1_list)\n",
    "        term3 = max_freq_ratio(i)\n",
    "        score_r6 = np.append(score_r6, term1*exp_ind_weight + term2*exp_ind_weight + term3)\n",
    "        \n",
    "    R6E_with_score = R6E_frequency.copy()\n",
    "    R6E_with_score[name+'_exp'] = score_r6\n",
    "    return R6E_with_score\n",
    "\n",
    "def quad_score_ctrl(set_1_percentile, ctrl_ind_weight, name):\n",
    "    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n",
    "    #using quadrumers in R6E for calculation \n",
    "    \n",
    "    #set 1 NOTE: using 99.5 percentile only gives 3 quadrumers\n",
    "    control_percentile = np.percentile(R2C_frequency['Weighted frequency'],set_1_percentile) \n",
    "    set1 = R6C_frequency[R6C_frequency['Weighted frequency'] > control_percentile]\n",
    "    set1_list = set1.index\n",
    "    \n",
    "    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n",
    "    set2 = R2C_frequency.merge(R6C_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2C freq', 'Weighted frequency_y':'R6C freq'})\n",
    "    set2['amp-fold value'] = set2['R6C freq']/set2['R2C freq']\n",
    "    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n",
    "    set2_list = set2.index\n",
    "    \n",
    "    score_r6 = []\n",
    "    for i in R6C_frequency.index:\n",
    "        term1 = (i in set1_list) or (i in set2_list)\n",
    "        term2 = (i in set1_list)\n",
    "        term3 = max_freq_ratio_ctrl(i)\n",
    "        score_r6 = np.append(score_r6, term1*ctrl_ind_weight + term2*ctrl_ind_weight + term3)\n",
    "        \n",
    "    R6C_with_score = R6C_frequency.copy()\n",
    "    R6C_with_score[name+'_ctrl'] = score_r6\n",
    "    return R6C_with_score\n",
    "\n",
    "def quad_score_full(set_1_percentile_exp, set_1_percentile_ctrl, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n",
    "    quad_exp = quad_score_exp(set_1_percentile_exp, exp_ind_weight, name)\n",
    "    quad_ctrl = quad_score_ctrl(set_1_percentile_ctrl, ctrl_ind_weight, name)\n",
    "    \n",
    "    merged = quad_exp.merge(quad_ctrl, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    merged[name] = exp_weight*merged[name+ '_exp'] - ctrl_weight*merged[name+'_ctrl']\n",
    "    return pd.DataFrame({'Weighted frequency': merged['Weighted frequency_x'], #weighted frequence is from R6E\n",
    "                        name : merged[name] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions for calculating aptamer score \n",
    "\n",
    "def aptamer_score(RnE, quad_score, name):\n",
    "    # Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n",
    "    # Inputs: RnE: a dataframe with 18-mer sequences;\n",
    "    #        quad_score: a df , out put of quad_score_full function,\n",
    "    \n",
    "    quadrumer_score = quad_score\n",
    "    score_name = quadrumer_score.columns[1]\n",
    "    \n",
    "    aptamer_score = []\n",
    "    aptamer_freqsum = []\n",
    "    for apt_seq in RnE['Trimed']:\n",
    "        all_quads = extract_quadrumers(apt_seq)\n",
    "        one_score = 0\n",
    "        one_freqsum = 0\n",
    "        for quad in all_quads:\n",
    "            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n",
    "                one_score += quadrumer_score.loc[quad][1]\n",
    "                one_freqsum += quadrumer_score.loc[quad][0]\n",
    "        aptamer_score = np.append(aptamer_score, one_score)\n",
    "        aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n",
    "        \n",
    "    tbl_with_score = RnE.copy().loc[:,['Trimed']]\n",
    "    tbl_with_score[name]=aptamer_score\n",
    "    tbl_with_score['Weighted frequency'] = aptamer_freqsum\n",
    "    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n",
    "    \n",
    "    \n",
    "    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n",
    "    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n",
    "\n",
    "\n",
    "    return tbl_with_score\n",
    "\n",
    "\n",
    "def aptamer_score_dFF(dFF_tbl, quad_score, name):\n",
    "    # for incorporating all 80 sequences of dFF table\n",
    "    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n",
    "    #Inputs: RnE: a dataframe with 18-mer sequences;\n",
    "    #        quad_score: a df , out put of quad_score_full function,\n",
    "    quadrumer_score = quad_score\n",
    "    score_name = quadrumer_score.columns[1]\n",
    "    \n",
    "    aptamer_score = []\n",
    "    aptamer_freqsum = []\n",
    "    for apt_seq in dFF_tbl['Sequence']:\n",
    "        all_quads = extract_quadrumers(apt_seq)\n",
    "        one_score = 0\n",
    "        #one_freqsum = 0\n",
    "        for quad in all_quads:\n",
    "            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n",
    "                one_score += quadrumer_score.loc[quad][1]\n",
    "                #one_freqsum += quadrumer_score.loc[quad][0]\n",
    "        aptamer_score = np.append(aptamer_score, one_score)\n",
    "        \n",
    "    tbl_with_score = dFF_tbl.copy().loc[:,['Name','Sequence','dFF']]\n",
    "    tbl_with_score[name]=aptamer_score\n",
    "    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n",
    "    \n",
    "    \n",
    "    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n",
    "    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n",
    "\n",
    "\n",
    "    return tbl_with_score\n",
    "\n",
    "def dFF_with_score(threshold, dFF_tbl, quad_score, score_name):\n",
    "    dFF_with_score = aptamer_score_dFF(dFF_tbl, quad_score, name=score_name)\n",
    "    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n",
    "    return dFF_with_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scorings\n",
    "\n",
    "$$\n",
    "Score_{quad,exp} ={\\color{red}{exp\\,ind\\,weight}} * I(quad\\in Set1\\,|\\,quad\\in Set2) + {\\color{red}{exp\\,ind\\,weight}} * I(quad\\in Set1) + max(\\frac{freq_{quad,j+1}}{freq_{quad}}|j\\in R)\n",
    "$$\n",
    "$$\n",
    "Score_{quad,ctrl} ={\\color{blue}{ctrl\\,ind\\,weight}} * I(quad\\in Set1\\,|\\,quad\\in Set2) + {\\color{blue}{ctrl\\,ind\\,weight}} * I(quad\\in Set1) + max(\\frac{freq_{quad,j+1}}{freq_{quad}}|j\\in R)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Score_{quad,final} = {\\color{brown}{exp\\,weight}}*Score_{quad,exp} - {\\color{green}{ctrl\\,weight}}*Score_{quad,ctrl}\n",
    "$$\n",
    "\n",
    "\n",
    "Set1: Kmers with frequencies once greater than 99.5th percentile of the kmers in the control round\n",
    "\n",
    "Set 2: with the same class size and consisting of kmers with the largest amplification- fold values was then defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70797f26e9cf424da335d2949e27aa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=99.5, description='exp_percentile', min=90.0, step=0.5, style=SliderSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n",
    "                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n",
    "                 exp_ind_weight = widgets.IntSlider(min=1,max=10,value=1,style={'description_width': '100px'}),\n",
    "                 ctrl_ind_weight = widgets.IntSlider(min=1,max=10,value=1,style={'description_width': '100px'}),\n",
    "                 exp_weight = widgets.IntSlider(min=1,max=10,value=1,style={'description_width': '100px'}),\n",
    "                 ctrl_weight = widgets.IntSlider(min=1,max=10,value=1,style={'description_width': '100px'}),\n",
    "                 name=widgets.Text(value='score name'),\n",
    "                 )\n",
    "\n",
    "def f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n",
    "    \n",
    "    threshold = 1.3\n",
    "    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight, name)\n",
    "    \n",
    "    dFF_1_with_score_tbl = dFF_with_score(threshold, dFF_tbl=dFF, quad_score=temp_quads_score, score_name=name)\n",
    "    dFF_2_with_score_tbl = dFF_with_score(threshold, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name=name)\n",
    "    dFF_3_with_score_tbl = dFF_with_score(threshold, dFF_tbl=dFF3_new, quad_score=temp_quads_score, score_name=name)\n",
    "    dFF_4P_with_score_tbl = dFF_with_score(threshold, dFF_tbl=dFF4_pos, quad_score=temp_quads_score, score_name=name)\n",
    "    dFF_4N_with_score_tbl = dFF_with_score(threshold, dFF_tbl=dFF4_neg, quad_score=temp_quads_score, score_name=name)\n",
    "    \n",
    "    dFF_1_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_1_with_score_tbl))]\n",
    "    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(len(dFF_2_with_score_tbl))]\n",
    "    dFF_3_with_score_tbl['Round'] = ['Round 3' for x in range(len(dFF_3_with_score_tbl))]\n",
    "    dFF_4P_with_score_tbl['Round'] = ['Round 4P' for x in range(len(dFF_4P_with_score_tbl))]\n",
    "    dFF_4N_with_score_tbl['Round'] = ['Round 4N' for x in range(len(dFF_4N_with_score_tbl))]\n",
    "    \n",
    "    dFF_combined = dFF_1_with_score_tbl.append(dFF_2_with_score_tbl).append(dFF_3_with_score_tbl).append(dFF_4P_with_score_tbl).append(dFF_4N_with_score_tbl)\n",
    "    \n",
    "    # outlier detection\n",
    "#     X_train = dFF_with_score_tbl.loc[:,['dFF',name]].values  #change training set, doesn't train on scores in su\n",
    "#     clf_list = [ABOD(),KNN()]\n",
    "\n",
    "#     for clf in clf_list:\n",
    "#         outlier_label = clf.fit(X_train).predict(X_train)\n",
    "#         dFF_with_outlier = dFF_with_score_tbl.copy()\n",
    "#         dFF_with_outlier['outlier'] = outlier_label\n",
    "#         outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n",
    "#         temp_r = outliers_dropped.corr().iloc[0,3]\n",
    "#         print('round 1 only r for ',str(clf)[:4],' is', temp_r)\n",
    "        \n",
    "    # calculate Pearson correlation coefficient\n",
    "    #    position [0,1] is the correlation between raw score with dFF values\n",
    "    #    position [0,3] is the correlation between score in su with dFF values -> wrong, shouldn't put Y and Xsu together\n",
    "\n",
    "    print('round 1 only correlation coefficent for all sequences is ', dFF_1_with_score_tbl.corr().iloc[0,1])\n",
    "    print('round 2 only correlation coefficent for all sequences is ', dFF_2_with_score_tbl.corr().iloc[0,1])\n",
    "    print('round 3 only correlation coefficent for all sequences is ', dFF_3_with_score_tbl.corr().iloc[0,1])\n",
    "    print('r1 and r2 correlation coefficent for all sequences is ', dFF_1_with_score_tbl.append(dFF_2_with_score_tbl).corr().iloc[0,1])\n",
    "    print('overall correlation coefficent for all sequences is ', dFF_combined.corr().iloc[0,1])\n",
    "    \n",
    "    # ploting\n",
    "    plt_title = \"{n} for R6E 18-mers, T={th}, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,th=threshold, exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n",
    "    \n",
    "    # update this line for plot in different scales\n",
    "    fig = px.scatter(dFF_combined, y=\"dFF\", x=name, symbol=\"Y/N\", color='Round', \n",
    "                     hover_name=\"Name\", hover_data=[\"Sequence\", \"dFF\", name])\n",
    "\n",
    "    \n",
    "    fig.update_layout(title_text=plt_title)\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSV  \n",
    "  \n",
    "Run the following cell and select desired parameters, then click run interact. The csv file will appear in the same folder that this notebook is located.  \n",
    "Output csv file has 4 columns: \n",
    "- raw score (X-axis)\n",
    "- dFF (Y-axis)\n",
    "- Round (label)\n",
    "- Name (for reference only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22d44979b8446d8c30327def7c5d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='exp_ind_weight', max=20, min=1, style=SliderStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n",
    "                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n",
    "                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n",
    "                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n",
    "                 )\n",
    "\n",
    "def f(exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight):\n",
    "    \n",
    "    temp_quads_score = quad_score_full(99.5, 99.5, exp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight, 'raw score')\n",
    "    \n",
    "    dFF_1_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF, quad_score=temp_quads_score, score_name='raw score')\n",
    "    dFF_2_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name='raw score')\n",
    "    dFF_3_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF3_new, quad_score=temp_quads_score, score_name='raw score')\n",
    "    dFF_4P_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF4_pos, quad_score=temp_quads_score, score_name='raw score')\n",
    "    dFF_4N_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF4_neg, quad_score=temp_quads_score, score_name='raw score')\n",
    "    \n",
    "    dFF_1_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_1_with_score_tbl))]\n",
    "    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(len(dFF_2_with_score_tbl))]\n",
    "    dFF_3_with_score_tbl['Round'] = ['Round 3' for x in range(len(dFF_3_with_score_tbl))]\n",
    "    dFF_4P_with_score_tbl['Round'] = ['Round 4P' for x in range(len(dFF_4P_with_score_tbl))]\n",
    "    dFF_4N_with_score_tbl['Round'] = ['Round 4N' for x in range(len(dFF_4N_with_score_tbl))]\n",
    "    \n",
    "    dFF_combined = dFF_1_with_score_tbl.append(dFF_2_with_score_tbl).append(dFF_3_with_score_tbl).append(dFF_4P_with_score_tbl).append(dFF_4N_with_score_tbl)\n",
    "    \n",
    "    # dFF_combined has columns : Name, Sequence, dFF, raw score, raw score percent, raw score su, Y/N, Round\n",
    "    \n",
    "    result = dFF_combined.loc[:,['raw score', 'dFF', 'Round','Name']]\n",
    "    file_name = \"E_ind={exp_ind_w}, C_ind={ctrl_ind_w}, E_w={exp}, C_w={ctrl}\".format(exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n",
    "    \n",
    "    result.to_csv(file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport ipywidgets as widgets\n%matplotlib inline\n\n\nplt.rcParams['figure.figsize'] = (14,8)\n#plt.rcParams['figure.dpi'] = 150\nsns.set()\nsns.set_context(\"talk\")",
    "id": "9aa4e29e43fd440d91ef16fd24a8527b",
    "idx": 2,
    "time": "2020-11-14T22:58:21.204Z",
    "type": "execution"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})",
    "id": "ab6c9dec94d54a03a3b71de8ff361625",
    "idx": 3,
    "time": "2020-11-14T22:58:21.453Z",
    "type": "execution"
   },
   {
    "code": "# functions that process raw data to get R6E and R6C\ndef max_freq_ratio(quad_seq):\n    #find the last term of the score definition\n    r2=R2E_frequency[R2E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2E_frequency.index else 0\n    r3=R3E_frequency[R3E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3E_frequency.index else 0\n    r4=R4E_frequency[R4E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4E_frequency.index else 0\n    r5=R5E_frequency[R5E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5E_frequency.index else 0      \n    r6=R6E_frequency[R6E_frequency.index==quad_seq]['Weighted frequency'][0]\n    \n    # handle the inf case\n    r6r5 = 0 if r5==0 else r6/r5\n    r5r4 = 0 if r4==0 else r5/r4\n    r4r3 = 0 if r3==0 else r4/r3\n    r3r2 = 0 if r3==0 else r3/r2\n    return max(r3r2,r4r3,r5r4,r6r5)\n\ndef max_freq_ratio_ctrl(quad_seq):\n    #find the last term of the score definition\n    r2=R2C_frequency[R2C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2C_frequency.index else 0\n    r3=R3C_frequency[R3C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3C_frequency.index else 0\n    r4=R4C_frequency[R4C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4C_frequency.index else 0\n    r5=R5C_frequency[R5C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5C_frequency.index else 0      \n    r6=R6C_frequency[R6C_frequency.index==quad_seq]['Weighted frequency'][0]\n    \n    # handle the inf case\n    r6r5 = 0 if r5==0 else r6/r5\n    r5r4 = 0 if r4==0 else r5/r4\n    r4r3 = 0 if r3==0 else r4/r3\n    r3r2 = 0 if r3==0 else r3/r2\n    \n    return max(r3r2,r4r3,r5r4,r6r5)\n\ndef extract_quadrumers(aptamer_sequence):\n    #takes in one 18-mer and return a table of quadrumers, with a position column and a quadrumer column\n    quadrumers = []\n    for i in np.arange(15):\n        quad = aptamer_sequence[i:i+4]\n        quadrumers = np.append(quadrumers,quad)\n    return quadrumers\n\n",
    "id": "cc4aa4c1fb0e41238a2c0c73bcf270b3",
    "idx": 4,
    "time": "2020-11-14T22:58:21.628Z",
    "type": "execution"
   },
   {
    "code": "# general function for calculating quad score\n\ndef quad_score_exp(set_1_percentile, exp_ind_weight, name):\n    #inputs: set_1_percentile -> see definition of set 1; {exp,ctrl}weight: weight for two indicator functions\n    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n    #using quadrumers in R6E for calculation \n    \n    #set 1: Kmers with frequencies once greater than 99.5th percentile of the kmers in the control round\n    control_percentile = np.percentile(R2E_frequency['Weighted frequency'], set_1_percentile)\n    set1 = R6E_frequency[R6E_frequency['Weighted frequency']>control_percentile]\n    set1_list = set1.index\n\n    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n    set2 = R2E_frequency.merge(R6E_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2E freq', 'Weighted frequency_y':'R6E freq'})\n    set2['amp-fold value'] = set2['R6E freq']/set2['R2E freq']\n    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n    set2_list = set2.index\n    \n    score_r6 = []\n    for i in R6E_frequency.index:\n        term1 = (i in set1_list) or (i in set2_list)\n        term2 = (i in set1_list)\n        term3 = max_freq_ratio(i)\n        score_r6 = np.append(score_r6,exp_ind_weight*term1 + exp_ind_weight*term2 + term3)\n        \n    R6E_with_score = R6E_frequency.copy()\n    R6E_with_score[name+'_exp'] = score_r6\n    return R6E_with_score\n\ndef quad_score_ctrl(set_1_percentile, ctrl_ind_weight, name):\n    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n    #using quadrumers in R6E for calculation \n    \n    #set 1 NOTE: using 99.5 percentile only gives 3 quadrumers\n    control_percentile = np.percentile(R2C_frequency['Weighted frequency'],set_1_percentile) \n    set1 = R6C_frequency[R6C_frequency['Weighted frequency'] > control_percentile]\n    set1_list = set1.index\n    \n    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n    set2 = R2C_frequency.merge(R6C_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2C freq', 'Weighted frequency_y':'R6C freq'})\n    set2['amp-fold value'] = set2['R6C freq']/set2['R2C freq']\n    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n    set2_list = set2.index\n    \n    score_r6 = []\n    for i in R6C_frequency.index:\n        term1 = (i in set1_list) or (i in set2_list)\n        term2 = (i in set1_list)\n        term3 = max_freq_ratio_ctrl(i)\n        score_r6 = np.append(score_r6, term1*ctrl_ind_weight + term2*ctrl_ind_weight + term3)\n        \n    R6C_with_score = R6C_frequency.copy()\n    R6C_with_score[name+'_ctrl'] = score_r6\n    return R6C_with_score\n\ndef quad_score_full(set_1_percentile_exp, set_1_percentile_ctrl, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    quad_exp = quad_score_exp(set_1_percentile_exp, exp_ind_weight, name)\n    quad_ctrl = quad_score_ctrl(set_1_percentile_ctrl, ctrl_ind_weight, name)\n    \n    merged = quad_exp.merge(quad_ctrl, how='left', left_index=True, right_index=True)\n    \n    merged[name] = exp_weight*merged[name+ '_exp'] - ctrl_weight*merged[name+'_ctrl']\n    return pd.DataFrame({'Weighted frequency': merged['Weighted frequency_x'], #weighted frequence is from R6E\n                        name : merged[name] })",
    "id": "ce70aeb1b9bb4fdc9ae3fcabd5a7b690",
    "idx": 5,
    "time": "2020-11-14T22:58:21.775Z",
    "type": "execution"
   },
   {
    "code": "# general functions for calculating aptamer score\n\ndef aptamer_score(RnE, quad_score, name):\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in RnE['Trimed']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n        \n    tbl_with_score = RnE.copy().loc[:,['Trimed']]\n    tbl_with_score[name]=aptamer_score\n    tbl_with_score['Weighted frequency'] = aptamer_freqsum\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n\n\ndef aptamer_score_dFF(dFF, quad_score, name):\n    # for incorporating all 80 sequences of dFF table\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in dFF['Sequence']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        #one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                #one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        #aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n        \n    tbl_with_score = dFF.copy().loc[:,['Name','Sequence','dFF']]\n    tbl_with_score[name]=aptamer_score\n    #tbl_with_score['Weighted frequency'] = aptamer_freqsum\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n",
    "id": "9d36d65c8b6b4dfd8c97cfcc365c876d",
    "idx": 6,
    "time": "2020-11-14T22:58:21.925Z",
    "type": "execution"
   },
   {
    "code": "# visualize related functions\ndef visualize_quad(df, name):\n    plt.figure(figsize=(9,6))\n    sns.scatterplot(x='Weighted frequency',y=name, data=df, s=20);\n    plt.title(name + ' for R6E quadrumers');\n    print('r for weighed frequency and '+name+' quadrumers is ', \n          df['Weighted frequency'].corr(df[name]))\n    \n    \ndef visualize_aptamer(df, name):\n    plt.figure(figsize=(9,6))\n    sns.scatterplot(x='Weighted frequency',y=name, data=df, s=20);\n    plt.title(name + ' for R6E 18-mers');\n    print('r for weighted frequency and '+ name +' aptamers',\n          df['Weighted frequency'].corr(df[name]))\n    \ndef visualize_scatter(dFF_with_score, score_name):\n    fig = px.scatter(dFF_with_score, y=\"dFF\", x=score_name+\" su\", text='Name', color=\"Y/N\")\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text='R6E in '+ score_name +', T=1.5')\n    fig.show()\n        ",
    "id": "c449f6b63f2e4ec089afabd17d9d2e37",
    "idx": 7,
    "time": "2020-11-14T22:58:22.097Z",
    "type": "execution"
   },
   {
    "code": "# distance related functions\ndef average_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum/len(setY)/len(setN)\n\ndef total_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum\n\n\ndef dFF_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef dFF_2_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF2_new, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef distance(dFF_df, score_name):\n    yes = dFF_df[dFF_df[\"Y/N\"]=='Y'].loc[:,['dFF',score_name+' su']]\n    no = dFF_df[dFF_df[\"Y/N\"]!='Y'].loc[:,['dFF',score_name+' su']]\n    distance = average_linkage(yes, no)\n    #distance = total_linkage(yes, no)\n    #print(\"distance for score 1 is \",distance)\n    return distance\n\n",
    "id": "49858156708f40358be35f4223115ed6",
    "idx": 8,
    "time": "2020-11-14T22:58:22.252Z",
    "type": "execution"
   },
   {
    "id": "9aa4e29e43fd440d91ef16fd24a8527b",
    "time": "2020-11-14T22:58:22.623Z",
    "type": "completion"
   },
   {
    "code": "from ipywidgets import interact_manual, interactive",
    "id": "d4706b0a0568438b8ce756b19f928fe8",
    "idx": 11,
    "time": "2020-11-14T22:58:23.024Z",
    "type": "execution"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name+' su']].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('r for ',str(clf)[:5],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name+' su', text='Name', symbol=\"Y/N\", color='Round')\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "09f4197890044adb874db49e43c50e58",
    "idx": 12,
    "time": "2020-11-14T22:58:23.552Z",
    "type": "execution"
   },
   {
    "id": "ab6c9dec94d54a03a3b71de8ff361625",
    "time": "2020-11-14T22:58:23.882Z",
    "type": "completion"
   },
   {
    "id": "cc4aa4c1fb0e41238a2c0c73bcf270b3",
    "time": "2020-11-14T22:58:23.893Z",
    "type": "completion"
   },
   {
    "id": "ce70aeb1b9bb4fdc9ae3fcabd5a7b690",
    "time": "2020-11-14T22:58:23.944Z",
    "type": "completion"
   },
   {
    "id": "9d36d65c8b6b4dfd8c97cfcc365c876d",
    "time": "2020-11-14T22:58:23.946Z",
    "type": "completion"
   },
   {
    "id": "c449f6b63f2e4ec089afabd17d9d2e37",
    "time": "2020-11-14T22:58:23.957Z",
    "type": "completion"
   },
   {
    "id": "49858156708f40358be35f4223115ed6",
    "time": "2020-11-14T22:58:23.979Z",
    "type": "completion"
   },
   {
    "id": "d4706b0a0568438b8ce756b19f928fe8",
    "time": "2020-11-14T22:58:23.995Z",
    "type": "completion"
   },
   {
    "id": "09f4197890044adb874db49e43c50e58",
    "time": "2020-11-14T22:58:24.194Z",
    "type": "completion"
   },
   {
    "code": "pip install pyod",
    "id": "738a25af935c4ccd965297953b346836",
    "idx": 0,
    "time": "2020-11-17T17:39:29.538Z",
    "type": "execution"
   },
   {
    "code": "from pyod.models.abod import ABOD\nfrom pyod.models.knn import KNN\nfrom pyod.models.ocsvm import OCSVM\nfrom pyod.models.pca import PCA\nfrom pyod.models.loci import LOCI\n\n",
    "id": "dcbba279c5b14c0d882fe3371cc0901f",
    "idx": 1,
    "time": "2020-11-17T17:39:30.553Z",
    "type": "execution"
   },
   {
    "code": "import csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport ipywidgets as widgets\n%matplotlib inline\n\n\nplt.rcParams['figure.figsize'] = (14,8)\n#plt.rcParams['figure.dpi'] = 150\nsns.set()\nsns.set_context(\"talk\")",
    "id": "165627e0378540308538086129e4bdfe",
    "idx": 2,
    "time": "2020-11-17T17:39:34.098Z",
    "type": "execution"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T17:39:34.431Z",
    "type": "execution"
   },
   {
    "code": "# functions that process raw data to get R6E and R6C\ndef max_freq_ratio(quad_seq):\n    #find the last term of the score definition\n    r2=R2E_frequency[R2E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2E_frequency.index else 0\n    r3=R3E_frequency[R3E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3E_frequency.index else 0\n    r4=R4E_frequency[R4E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4E_frequency.index else 0\n    r5=R5E_frequency[R5E_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5E_frequency.index else 0      \n    r6=R6E_frequency[R6E_frequency.index==quad_seq]['Weighted frequency'][0]\n    \n    # handle the inf case\n    r6r5 = 0 if r5==0 else r6/r5\n    r5r4 = 0 if r4==0 else r5/r4\n    r4r3 = 0 if r3==0 else r4/r3\n    r3r2 = 0 if r3==0 else r3/r2\n    return max(r3r2,r4r3,r5r4,r6r5)\n\ndef max_freq_ratio_ctrl(quad_seq):\n    #find the last term of the score definition\n    r2=R2C_frequency[R2C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R2C_frequency.index else 0\n    r3=R3C_frequency[R3C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R3C_frequency.index else 0\n    r4=R4C_frequency[R4C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R4C_frequency.index else 0\n    r5=R5C_frequency[R5C_frequency.index==quad_seq]['Weighted frequency'][0] if quad_seq in R5C_frequency.index else 0      \n    r6=R6C_frequency[R6C_frequency.index==quad_seq]['Weighted frequency'][0]\n    \n    # handle the inf case\n    r6r5 = 0 if r5==0 else r6/r5\n    r5r4 = 0 if r4==0 else r5/r4\n    r4r3 = 0 if r3==0 else r4/r3\n    r3r2 = 0 if r3==0 else r3/r2\n    \n    return max(r3r2,r4r3,r5r4,r6r5)\n\ndef extract_quadrumers(aptamer_sequence):\n    #takes in one 18-mer and return a table of quadrumers, with a position column and a quadrumer column\n    quadrumers = []\n    for i in np.arange(15):\n        quad = aptamer_sequence[i:i+4]\n        quadrumers = np.append(quadrumers,quad)\n    return quadrumers\n\n",
    "id": "cbf2d2efcf5a4fd786e6801f0856fd15",
    "idx": 4,
    "time": "2020-11-17T17:39:35.843Z",
    "type": "execution"
   },
   {
    "code": "# general function for calculating quad score\n\ndef quad_score_exp(set_1_percentile, exp_ind_weight, name):\n    #inputs: set_1_percentile -> see definition of set 1; {exp,ctrl}weight: weight for two indicator functions\n    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n    #using quadrumers in R6E for calculation \n    \n    #set 1: Kmers with frequencies once greater than 99.5th percentile of the kmers in the control round\n    control_percentile = np.percentile(R2E_frequency['Weighted frequency'], set_1_percentile)\n    set1 = R6E_frequency[R6E_frequency['Weighted frequency']>control_percentile]\n    set1_list = set1.index\n\n    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n    set2 = R2E_frequency.merge(R6E_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2E freq', 'Weighted frequency_y':'R6E freq'})\n    set2['amp-fold value'] = set2['R6E freq']/set2['R2E freq']\n    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n    set2_list = set2.index\n    \n    score_r6 = []\n    for i in R6E_frequency.index:\n        term1 = (i in set1_list) or (i in set2_list)\n        term2 = (i in set1_list)\n        term3 = max_freq_ratio(i)\n        score_r6 = np.append(score_r6,exp_ind_weight*term1 + exp_ind_weight*term2 + term3)\n        \n    R6E_with_score = R6E_frequency.copy()\n    R6E_with_score[name+'_exp'] = score_r6\n    return R6E_with_score\n\ndef quad_score_ctrl(set_1_percentile, ctrl_ind_weight, name):\n    #return a dataframe with quadrumer as index and weighted frequency and scores as two columns\n    #using quadrumers in R6E for calculation \n    \n    #set 1 NOTE: using 99.5 percentile only gives 3 quadrumers\n    control_percentile = np.percentile(R2C_frequency['Weighted frequency'],set_1_percentile) \n    set1 = R6C_frequency[R6C_frequency['Weighted frequency'] > control_percentile]\n    set1_list = set1.index\n    \n    #set 2: : with the same class size and consisting of kmers with the largest amplification-fold values was then defined. \n    set2 = R2C_frequency.merge(R6C_frequency,on='Quadrumer').rename(columns={'Weighted frequency_x':'R2C freq', 'Weighted frequency_y':'R6C freq'})\n    set2['amp-fold value'] = set2['R6C freq']/set2['R2C freq']\n    set2 = set2.sort_values('amp-fold value', ascending=False).head(len(set1))\n    set2_list = set2.index\n    \n    score_r6 = []\n    for i in R6C_frequency.index:\n        term1 = (i in set1_list) or (i in set2_list)\n        term2 = (i in set1_list)\n        term3 = max_freq_ratio_ctrl(i)\n        score_r6 = np.append(score_r6, term1*ctrl_ind_weight + term2*ctrl_ind_weight + term3)\n        \n    R6C_with_score = R6C_frequency.copy()\n    R6C_with_score[name+'_ctrl'] = score_r6\n    return R6C_with_score\n\ndef quad_score_full(set_1_percentile_exp, set_1_percentile_ctrl, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    quad_exp = quad_score_exp(set_1_percentile_exp, exp_ind_weight, name)\n    quad_ctrl = quad_score_ctrl(set_1_percentile_ctrl, ctrl_ind_weight, name)\n    \n    merged = quad_exp.merge(quad_ctrl, how='left', left_index=True, right_index=True)\n    \n    merged[name] = exp_weight*merged[name+ '_exp'] - ctrl_weight*merged[name+'_ctrl']\n    return pd.DataFrame({'Weighted frequency': merged['Weighted frequency_x'], #weighted frequence is from R6E\n                        name : merged[name] })",
    "id": "a0eae977fdae4436a9f7cd76c6c853fa",
    "idx": 5,
    "time": "2020-11-17T17:39:36.062Z",
    "type": "execution"
   },
   {
    "code": "# general functions for calculating aptamer score\n\ndef aptamer_score(RnE, quad_score, name):\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in RnE['Trimed']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n        \n    tbl_with_score = RnE.copy().loc[:,['Trimed']]\n    tbl_with_score[name]=aptamer_score\n    tbl_with_score['Weighted frequency'] = aptamer_freqsum\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n\n\ndef aptamer_score_dFF(dFF, quad_score, name):\n    # for incorporating all 80 sequences of dFF table\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in dFF['Sequence']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        #one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                #one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        #aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n        \n    tbl_with_score = dFF.copy().loc[:,['Name','Sequence','dFF']]\n    tbl_with_score[name]=aptamer_score\n    #tbl_with_score['Weighted frequency'] = aptamer_freqsum\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n",
    "id": "57f63315df8943b98c100d3227f073be",
    "idx": 6,
    "time": "2020-11-17T17:39:36.257Z",
    "type": "execution"
   },
   {
    "code": "# visualize related functions\ndef visualize_quad(df, name):\n    plt.figure(figsize=(9,6))\n    sns.scatterplot(x='Weighted frequency',y=name, data=df, s=20);\n    plt.title(name + ' for R6E quadrumers');\n    print('r for weighed frequency and '+name+' quadrumers is ', \n          df['Weighted frequency'].corr(df[name]))\n    \n    \ndef visualize_aptamer(df, name):\n    plt.figure(figsize=(9,6))\n    sns.scatterplot(x='Weighted frequency',y=name, data=df, s=20);\n    plt.title(name + ' for R6E 18-mers');\n    print('r for weighted frequency and '+ name +' aptamers',\n          df['Weighted frequency'].corr(df[name]))\n    \ndef visualize_scatter(dFF_with_score, score_name):\n    fig = px.scatter(dFF_with_score, y=\"dFF\", x=score_name+\" su\", text='Name', color=\"Y/N\")\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text='R6E in '+ score_name +', T=1.5')\n    fig.show()\n        ",
    "id": "7f51fdf8a5a34bea82e29793e2dcc576",
    "idx": 7,
    "time": "2020-11-17T17:39:36.434Z",
    "type": "execution"
   },
   {
    "code": "# distance related functions\ndef average_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum/len(setY)/len(setN)\n\ndef total_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum\n\n\ndef dFF_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef dFF_2_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF2_new, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef distance(dFF_df, score_name):\n    yes = dFF_df[dFF_df[\"Y/N\"]=='Y'].loc[:,['dFF',score_name+' su']]\n    no = dFF_df[dFF_df[\"Y/N\"]!='Y'].loc[:,['dFF',score_name+' su']]\n    distance = average_linkage(yes, no)\n    #distance = total_linkage(yes, no)\n    #print(\"distance for score 1 is \",distance)\n    return distance\n\n",
    "id": "cba5183d1f3d4c54a208ccdedcd75102",
    "idx": 8,
    "time": "2020-11-17T17:39:36.615Z",
    "type": "execution"
   },
   {
    "id": "738a25af935c4ccd965297953b346836",
    "time": "2020-11-17T17:39:39.150Z",
    "type": "completion"
   },
   {
    "id": "dcbba279c5b14c0d882fe3371cc0901f",
    "time": "2020-11-17T17:39:39.983Z",
    "type": "completion"
   },
   {
    "id": "165627e0378540308538086129e4bdfe",
    "time": "2020-11-17T17:39:40.992Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name+' su']].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('r for ',str(clf)[:5],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name+' su', text='Name', symbol=\"Y/N\", color='Round')\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T17:39:42.284Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T17:39:42.363Z",
    "type": "completion"
   },
   {
    "id": "cbf2d2efcf5a4fd786e6801f0856fd15",
    "time": "2020-11-17T17:39:42.380Z",
    "type": "completion"
   },
   {
    "id": "a0eae977fdae4436a9f7cd76c6c853fa",
    "time": "2020-11-17T17:39:42.434Z",
    "type": "completion"
   },
   {
    "id": "57f63315df8943b98c100d3227f073be",
    "time": "2020-11-17T17:39:42.461Z",
    "type": "completion"
   },
   {
    "id": "7f51fdf8a5a34bea82e29793e2dcc576",
    "time": "2020-11-17T17:39:42.482Z",
    "type": "completion"
   },
   {
    "id": "cba5183d1f3d4c54a208ccdedcd75102",
    "time": "2020-11-17T17:39:42.501Z",
    "type": "completion"
   },
   {
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "time": "2020-11-17T17:39:42.719Z",
    "type": "completion"
   },
   {
    "code": "from ipywidgets import interact_manual, interactive",
    "id": "393ea939a4d546f68680be05a99cbe03",
    "idx": 11,
    "time": "2020-11-17T17:39:46.176Z",
    "type": "execution"
   },
   {
    "id": "393ea939a4d546f68680be05a99cbe03",
    "time": "2020-11-17T17:39:46.244Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name+' su']].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('r for ',str(clf)[:5],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name+' su', text='Name', symbol=\"Y/N\", color='Round')\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T17:39:46.549Z",
    "type": "execution"
   },
   {
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "time": "2020-11-17T17:39:46.850Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 16,
    "time": "2020-11-17T17:40:48.580Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T17:40:56.358Z",
    "type": "completion"
   },
   {
    "code": "dFF_2_with_score_tbl",
    "id": "82c4f1ebe8b145628742fa43505456e5",
    "idx": 17,
    "time": "2020-11-17T17:41:30.626Z",
    "type": "execution"
   },
   {
    "id": "82c4f1ebe8b145628742fa43505456e5",
    "time": "2020-11-17T17:41:30.743Z",
    "type": "completion"
   },
   {
    "code": "dFF2",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:41:34.784Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:41:34.901Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:41:49.807Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:41:49.904Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score.join(dFF_2_with_score_tbl,left_on='Trimed',right_on='Sequence',how='inner')",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:43:59.680Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:43:59.849Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score.join(dFF_2_with_score_tbl,'Trimed','Sequence',how='inner')",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:44:09.194Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:44:09.296Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score.merge(dFF_2_with_score_tbl,left_on='Trimed',right_on='Sequence',how='inner')",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:44:53.101Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:44:53.220Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score.merge(dFF2,left_on='Trimed',right_on='Sequence',how='inner')",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 18,
    "time": "2020-11-17T17:45:09.895Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:45:09.995Z",
    "type": "completion"
   },
   {
    "code": "temp_quads_score",
    "id": "effb94e2b00447058be5a7d9aef6143a",
    "idx": 17,
    "time": "2020-11-17T17:50:30.031Z",
    "type": "execution"
   },
   {
    "id": "effb94e2b00447058be5a7d9aef6143a",
    "time": "2020-11-17T17:50:30.351Z",
    "type": "completion"
   },
   {
    "code": "apt_seq='ACAACCCAACTCCGCTCG'\nall_quads = extract_quadrumers(apt_seq)\none_score = 0\nfor quad in all_quads:\n    if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n        one_score += quadrumer_score.loc[quad][1]\none_score",
    "id": "af4d6247d4f94d7b81d630903926d623",
    "idx": 18,
    "time": "2020-11-17T17:52:16.444Z",
    "type": "execution"
   },
   {
    "id": "af4d6247d4f94d7b81d630903926d623",
    "time": "2020-11-17T17:52:16.533Z",
    "type": "completion"
   },
   {
    "code": "apt_seq='ACAACCCAACTCCGCTCG'\nall_quads = extract_quadrumers(apt_seq)\none_score = 0\nfor quad in all_quads:\n    if len(temp_quads_score[temp_quads_score.index==quad]) != 0:\n        one_score += quadrumer_score.loc[quad][1]\none_score",
    "id": "af4d6247d4f94d7b81d630903926d623",
    "idx": 18,
    "time": "2020-11-17T17:52:30.963Z",
    "type": "execution"
   },
   {
    "id": "af4d6247d4f94d7b81d630903926d623",
    "time": "2020-11-17T17:52:31.085Z",
    "type": "completion"
   },
   {
    "code": "apt_seq='ACAACCCAACTCCGCTCG'\nall_quads = extract_quadrumers(apt_seq)\none_score = 0\nfor quad in all_quads:\n    if len(temp_quads_score[temp_quads_score.index==quad]) != 0:\n        one_score += temp_quads_score.loc[quad][1]\none_score",
    "id": "af4d6247d4f94d7b81d630903926d623",
    "idx": 18,
    "time": "2020-11-17T17:52:33.988Z",
    "type": "execution"
   },
   {
    "id": "af4d6247d4f94d7b81d630903926d623",
    "time": "2020-11-17T17:52:34.108Z",
    "type": "completion"
   },
   {
    "code": "temp_aptamer_score.merge(dFF2,left_on='Trimed',right_on='Sequence',how='inner').sort_values('Sequence')",
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "idx": 20,
    "time": "2020-11-17T17:56:25.538Z",
    "type": "execution"
   },
   {
    "id": "d2a60dc7f4674107840e8f3ff190cc71",
    "time": "2020-11-17T17:56:25.708Z",
    "type": "completion"
   },
   {
    "code": "dFF_2_with_score_tbl.sort_values('Sequence')",
    "id": "82c4f1ebe8b145628742fa43505456e5",
    "idx": 19,
    "time": "2020-11-17T17:56:28.646Z",
    "type": "execution"
   },
   {
    "id": "82c4f1ebe8b145628742fa43505456e5",
    "time": "2020-11-17T17:56:28.754Z",
    "type": "completion"
   },
   {
    "code": "# general functions for calculating aptamer score\n\ndef aptamer_score(RnE, quad_score, name):\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in RnE['Trimed']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        aptamer_freqsum = np.append(aptamer_freqsum, one_freqsum)\n        \n    tbl_with_score = RnE.copy().loc[:,['Trimed']]\n    tbl_with_score[name]=aptamer_score\n    tbl_with_score['Weighted frequency'] = aptamer_freqsum\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n\n\ndef aptamer_score_dFF(dFF_tbl, quad_score, name):\n    # for incorporating all 80 sequences of dFF table\n    #Returns a dataframe like R6E, with 18-mer sequence and score for each aptamer\n    #Inputs: RnE: a dataframe with 18-mer sequences;\n    #        quad_score: a df , out put of quad_score_full function,\n    quadrumer_score = quad_score\n    score_name = quadrumer_score.columns[1]\n    \n    aptamer_score = []\n    aptamer_freqsum = []\n    for apt_seq in dFF_tbl['Sequence']:\n        all_quads = extract_quadrumers(apt_seq)\n        one_score = 0\n        #one_freqsum = 0\n        for quad in all_quads:\n            if len(quadrumer_score[quadrumer_score.index==quad]) != 0:\n                one_score += quadrumer_score.loc[quad][1]\n                #one_freqsum += quadrumer_score.loc[quad][0]\n        aptamer_score = np.append(aptamer_score, one_score)\n        \n    tbl_with_score = dFF_tbl.copy().loc[:,['Name','Sequence','dFF']]\n    tbl_with_score[name]=aptamer_score\n    tbl_with_score.index = tbl_with_score.index + 1 # reset index to match!\n    \n    \n    tbl_with_score[name+' percent'] = 100*tbl_with_score[name]/max(tbl_with_score[name])\n    tbl_with_score[name+' su'] = (tbl_with_score[name]-np.mean(tbl_with_score[name]))/np.std(tbl_with_score[name])\n\n\n    return tbl_with_score\n",
    "id": "57f63315df8943b98c100d3227f073be",
    "idx": 6,
    "time": "2020-11-17T18:03:56.200Z",
    "type": "execution"
   },
   {
    "id": "57f63315df8943b98c100d3227f073be",
    "time": "2020-11-17T18:03:56.285Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    \n    #change training set, doesn't train on scores in su.\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name]].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('round 1 only r for ',str(clf)[:4],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('round 1 only correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    \n    # update this line for plot in different scales\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name, text='Name', symbol=\"Y/N\", color='Round')\n    \n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T18:10:16.020Z",
    "type": "execution"
   },
   {
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "time": "2020-11-17T18:10:16.476Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    \n    #change training set, doesn't train on scores in su.\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name]].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('round 1 only r for ',str(clf)[:4],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('round 1 only correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    \n    # update this line for plot in different scales\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name+' su', text='Name', symbol=\"Y/N\", color='Round')\n    \n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T18:12:29.567Z",
    "type": "execution"
   },
   {
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "time": "2020-11-17T18:12:29.918Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_combined = dFF_with_score_tbl.copy().append(dFF_2_with_score_tbl)\n    \n    # outlier detection\n    \n    #change training set, doesn't train on scores in su.\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name]].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('round 1 only r for ',str(clf)[:4],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('round 1 only correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    \n    # update this line for plot in different scales\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name, text='Name', symbol=\"Y/N\", color='Round')\n    \n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T18:17:42.776Z",
    "type": "execution"
   },
   {
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "time": "2020-11-17T18:17:43.131Z",
    "type": "completion"
   },
   {
    "code": "dFF2",
    "id": "19b92cb827884cc2b71e9ba1966934d2",
    "idx": 16,
    "time": "2020-11-17T18:25:00.811Z",
    "type": "execution"
   },
   {
    "id": "19b92cb827884cc2b71e9ba1966934d2",
    "time": "2020-11-17T18:25:01.068Z",
    "type": "completion"
   },
   {
    "code": "dFF2_new",
    "id": "19b92cb827884cc2b71e9ba1966934d2",
    "idx": 16,
    "time": "2020-11-17T18:25:07.317Z",
    "type": "execution"
   },
   {
    "id": "19b92cb827884cc2b71e9ba1966934d2",
    "time": "2020-11-17T18:25:07.418Z",
    "type": "completion"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx',i).loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\ndFF3 = pd.read_csv('dFF3.csv')",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T18:32:12.524Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T18:32:13.220Z",
    "type": "completion"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx',i).loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\ndFF3 = pd.read_csv('dFF3.csv')",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T18:32:28.212Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T18:32:28.731Z",
    "type": "completion"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\ndFF3 = pd.read_csv('dFF3.csv')",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T18:32:40.806Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T18:32:41.215Z",
    "type": "completion"
   },
   {
    "code": "dFF3",
    "id": "1deca70459304656a01a12696e1da879",
    "idx": 4,
    "time": "2020-11-17T18:32:51.229Z",
    "type": "execution"
   },
   {
    "id": "1deca70459304656a01a12696e1da879",
    "time": "2020-11-17T18:32:51.357Z",
    "type": "completion"
   },
   {
    "code": "# distance related functions\ndef average_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum/len(setY)/len(setN)\n\ndef total_linkage(setY,setN):\n    #input: set1 and set2 are two dataframe with x-value as scores and y-value as dFF\n    #output: returnn a average linkage(distance) between two groups of points\n    sum = 0\n    for i in np.arange(len(setY)):\n        x1=setY.iloc[i][1]\n        y1=setY.iloc[i][0]\n        for j in np.arange(len(setN)):\n            x2= setN.iloc[j][1]\n            y2=setN.iloc[j][0]\n            sum += np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return sum\n\n\ndef dFF_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef dFF_2_with_score(threshold, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF2_new, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n\ndef dFF_with_score(threshold, dFF_tbl, quad_score, score_name):\n    dFF_with_score = aptamer_score_dFF(dFF_tbl, quad_score, name=score_name)\n    dFF_with_score['Y/N'] = dFF_with_score['dFF'].map(lambda x: 'Y' if x>threshold else 'N')\n    return dFF_with_score\n    \ndef distance(dFF_df, score_name):\n    yes = dFF_df[dFF_df[\"Y/N\"]=='Y'].loc[:,['dFF',score_name+' su']]\n    no = dFF_df[dFF_df[\"Y/N\"]!='Y'].loc[:,['dFF',score_name+' su']]\n    distance = average_linkage(yes, no)\n    #distance = total_linkage(yes, no)\n    #print(\"distance for score 1 is \",distance)\n    return distance\n\n",
    "id": "cba5183d1f3d4c54a208ccdedcd75102",
    "idx": 8,
    "time": "2020-11-17T18:35:01.844Z",
    "type": "execution"
   },
   {
    "id": "cba5183d1f3d4c54a208ccdedcd75102",
    "time": "2020-11-17T18:35:01.928Z",
    "type": "completion"
   },
   {
    "code": "dFF_2_with_score_tbl",
    "id": "bdd86d9283af47f1aad0ce04faaa75d1",
    "idx": 16,
    "time": "2020-11-17T18:35:13.194Z",
    "type": "execution"
   },
   {
    "id": "bdd86d9283af47f1aad0ce04faaa75d1",
    "time": "2020-11-17T18:35:13.311Z",
    "type": "completion"
   },
   {
    "code": "dFF_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name=name)",
    "id": "4901cace7f754a21954d6d54dc836c8c",
    "idx": 17,
    "time": "2020-11-17T18:36:07.813Z",
    "type": "execution"
   },
   {
    "id": "4901cace7f754a21954d6d54dc836c8c",
    "time": "2020-11-17T18:36:07.981Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 15,
    "time": "2020-11-17T18:39:56.605Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T18:40:03.759Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, dFF, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, dFF2_new quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 15,
    "time": "2020-11-17T18:40:17.130Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T18:40:17.201Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, dFF, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, dFF2_new, quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 15,
    "time": "2020-11-17T18:40:21.188Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T18:40:21.262Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_2_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 15,
    "time": "2020-11-17T18:40:47.108Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T18:40:54.538Z",
    "type": "completion"
   },
   {
    "code": "# parameters\nexp_percentile, ctrl_percentile = [99.5, 99.5]\nexp_ind_weight, ctrl_ind_weight,exp_weight, ctrl_weight = [4 ,9, 9, 6]\nname = 'score name'\n\ntemp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name)\ntemp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\ndFF_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF, quad_score=temp_quads_score, score_name=name)\ndFF_2_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name=name)\n",
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "idx": 15,
    "time": "2020-11-17T18:40:55.154Z",
    "type": "execution"
   },
   {
    "id": "3e1b2afcb9724c70a6d94fe0c9b71e6c",
    "time": "2020-11-17T18:41:02.919Z",
    "type": "completion"
   },
   {
    "code": "dFF_with_score(threshold=1.5, dFF_tbl=dFF3, quad_score=temp_quads_score, score_name=name)",
    "id": "cdba6e304444484688cccc83d656ae9e",
    "idx": 16,
    "time": "2020-11-17T18:41:09.595Z",
    "type": "execution"
   },
   {
    "id": "cdba6e304444484688cccc83d656ae9e",
    "time": "2020-11-17T18:41:09.999Z",
    "type": "completion"
   },
   {
    "code": "dFF3",
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "idx": 4,
    "time": "2020-11-17T18:41:35.251Z",
    "type": "execution"
   },
   {
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "time": "2020-11-17T18:41:35.364Z",
    "type": "completion"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\ndFF3 = pd.read_csv('dFF3.csv',,usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T18:41:58.105Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T18:41:58.175Z",
    "type": "completion"
   },
   {
    "code": "dFF3",
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "idx": 4,
    "time": "2020-11-17T18:41:59.486Z",
    "type": "execution"
   },
   {
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "time": "2020-11-17T18:41:59.599Z",
    "type": "completion"
   },
   {
    "code": "#import raw data\nfor i in np.arange(2,7):\n    exec(\"R{}E = pd.read_csv('Serotonin-data/{}RE.csv')\".format(i, i))\n    exec(\"R{}C = pd.read_csv('Serotonin-data/{}RC.csv')\".format(i, i))\n\n#import processed data\nfor i in np.arange(2,7):\n    exec(\"R{}E_frequency = pd.read_csv('Serotonin-processed-data/R{}E_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_frequency = pd.read_csv('Serotonin-processed-data/R{}C_frequency.csv',index_col=0)\".format(i, i))\n    exec(\"R{}E_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}E_full_table_weighted.csv',index_col=0)\".format(i, i))\n    exec(\"R{}C_full_table_weighted = pd.read_csv('Serotonin-processed-data/R{}C_full_table_weighted.csv',index_col=0)\".format(i, i))\n    \ndFF = pd.read_csv('dFF table.csv',usecols=[0,1,2])\ndFF2 = pd.read_excel('dFF2.xlsx').loc[:,['df/f','Trimed']].rename(columns={'Trimed':'Sequence'})\ndFF2_new = pd.DataFrame({'Name':[\"N/A\" for x in range(10)], 'Sequence':dFF2['Sequence'], 'dFF':dFF2['df/f']})\ndFF3 = pd.read_csv('dFF3.csv',usecols=[0,1,3]).rename(columns={'dFF_1195':'dFF'}) #use dFF value at 1195nm\n",
    "id": "36997de9abf641649620fb5fa596bbfa",
    "idx": 3,
    "time": "2020-11-17T18:42:05.056Z",
    "type": "execution"
   },
   {
    "id": "36997de9abf641649620fb5fa596bbfa",
    "time": "2020-11-17T18:42:05.367Z",
    "type": "completion"
   },
   {
    "code": "dFF3",
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "idx": 4,
    "time": "2020-11-17T18:42:07.375Z",
    "type": "execution"
   },
   {
    "id": "092535bdcb744a9c8db511c1da105ffc",
    "time": "2020-11-17T18:42:07.455Z",
    "type": "completion"
   },
   {
    "code": "dFF_with_score(threshold=1.5, dFF_tbl=dFF3, quad_score=temp_quads_score, score_name=name)",
    "id": "cdba6e304444484688cccc83d656ae9e",
    "idx": 16,
    "time": "2020-11-17T18:42:23.279Z",
    "type": "execution"
   },
   {
    "id": "cdba6e304444484688cccc83d656ae9e",
    "time": "2020-11-17T18:42:23.510Z",
    "type": "completion"
   },
   {
    "code": "@interact_manual(exp_percentile= widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 ctrl_percentile = widgets.FloatSlider(min=90,max=100,step=0.5,value=99.5,style={'description_width': '100px'}),\n                 exp_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_ind_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 exp_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 ctrl_weight = widgets.IntSlider(min=1,max=20,value=1,style={'description_width': '100px'}),\n                 name=widgets.Text(value='score name'),\n                 )\n\ndef f(exp_percentile, ctrl_percentile, exp_ind_weight, ctrl_ind_weight, exp_weight, ctrl_weight, name):\n    temp_quads_score = quad_score_full(exp_percentile, ctrl_percentile, \n                                       exp_ind_weight, ctrl_ind_weight, \n                                       exp_weight, ctrl_weight, name)\n    \n    # temp_aptamer_score = aptamer_score(R6E, temp_quads_score, name)\n    dFF_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF, quad_score=temp_quads_score, score_name=name)\n    dFF_2_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF2_new, quad_score=temp_quads_score, score_name=name)\n    dFF_3_with_score_tbl = dFF_with_score(threshold=1.5, dFF_tbl=dFF3, quad_score=temp_quads_score, score_name=name)\n    \n    dFF_with_score_tbl['Round'] = ['Round 1' for x in range(len(dFF_with_score_tbl))]\n    dFF_2_with_score_tbl['Round'] = ['Round 2' for x in range(10)]\n    dFF_3_with_score_tbl['Round'] = ['Round 3' for x in range(len(dFF_3_with_score_tbl))]\n    \n    \n    dFF_combined = dFF_with_score_tbl.append(dFF_2_with_score_tbl).append(dFF_3_with_score_tbl)\n    \n    # outlier detection\n    \n    #change training set, doesn't train on scores in su.\n    X_train = dFF_with_score_tbl.loc[:,['dFF',name]].values\n    clf_list = [ABOD(),KNN()]\n\n    for clf in clf_list:\n        outlier_label = clf.fit(X_train).predict(X_train)\n        dFF_with_outlier = dFF_with_score_tbl.copy()\n        dFF_with_outlier['outlier'] = outlier_label\n        outliers_dropped = dFF_with_outlier[dFF_with_outlier['outlier']==0]\n        temp_r = outliers_dropped.corr().iloc[0,3]\n        print('round 1 only r for ',str(clf)[:4],' is', temp_r)\n        \n    # overall correlation coefficient\n    r3 = dFF_with_score_tbl.corr().iloc[0,3]\n    print('round 1 only correlation coefficent for all sequences is ', r3)\n    \n    \n    # distance\n    print('distance is ',distance(dFF_df=dFF_with_score_tbl, score_name=name))\n    \n    # ploting\n    plt_title = \"{n} for R6E 18-mers, T=1.5, exp_ind_w = {exp_ind_w}, ctrl_ind_w = {ctrl_ind_w}, exp_w = {exp}, ctrl_w={ctrl}\".format(n=name,exp_ind_w=exp_ind_weight,ctrl_ind_w=ctrl_ind_weight,exp =exp_weight,ctrl=ctrl_weight)\n    \n    # update this line for plot in different scales\n    fig = px.scatter(dFF_combined, y=\"dFF\", x=name, text='Name', symbol=\"Y/N\", color='Round')\n    \n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=plt_title)\n    fig.show()\n    \n    #return distance(dFF_df=dFF_with_score_tbl, score_name=name)",
    "id": "1861ae4846004d69a784c7f4db4bfc86",
    "idx": 12,
    "time": "2020-11-17T18:43:39.445Z",
    "type": "execution"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
